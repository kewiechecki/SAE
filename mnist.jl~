# A toy model of superposition to demonstrate feature splitting.
# An MNIST classifier is trained with a 3-neuron bottleneck.
# This forces the model to represent more than 3 features in this layer.
# Our goal is to split the activations of this layer into linearly separable features. 

include("trainingfns.jl")

using MLDatasets, StatsPlots, OneHotArrays
using Flux: logitcrossentropy

using JLD2,Tables,CSV

# where to write the toy model
path = "data/MNIST/"

epochs = 100
batchsize=512

η = 0.001
λ = 0.001
opt = Flux.Optimiser(Flux.AdamW(η),Flux.WeightDecay(λ))

dat = MNIST(split=:train)[:]
target = onehotbatch(dat.targets,0:9)

m_x,m_y,n = size(dat.features)
X = reshape(dat.features[:, :, :], m_x, m_y, 1, n)

loader = Flux.DataLoader((X,target),
                         batchsize=batchsize,
                         shuffle=true)

kern = (3,3)
s = (2,2)
θ_conv = Chain(Conv(kern,1 => 3,relu,stride=s),
               Conv(kern,3 => 6,relu,stride=s),
               Conv(kern,6 => 9,relu,stride=s),
               Conv((2,2),9 => 12,relu))

θ_mlp = Chain(Dense(12 => 6,relu),
              Dense(6 => m,relu))

θ_outer = Chain(θ_conv,
                x->reshape(x,12,:),
                θ_mlp)
               
π_outer = Chain(Dense(m => 5,relu),
                Dense(5 => 10,relu),
                softmax)

M_outer = Chain(θ_outer,π_outer) |> gpu

sae = SAE(m,d) |> gpu

L_outer = []

train!(M_outer,loader,opt,epochs,logitcrossentropy,L_outer);

state_outer = Flux.state(M_outer) |> cpu;
jldsave(path*"state_outer.jld2";state_outer)
Tables.table(L_outer) |> CSV.write(path*"L_outer.csv")
